{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11604688",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chat_completion, yesno_completion\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from llm import chat_completion, yesno_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46755780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with desired dataset (from HuggingFace)\n",
    "hf_dataset_name = \"hendrycks/competition_math\"\n",
    "dataset = load_dataset(hf_dataset_name)\n",
    "\n",
    "# Example item from \"hendrycks/competition_math\"\n",
    "\"\"\"\n",
    "{'problem': 'A board game spinner is divided into three parts labeled $A$, $B$  and $C$. The probability of the spinner landing on $A$ is $\\\\frac{1}{3}$ and the probability of the spinner landing on $B$ is $\\\\frac{5}{12}$.  What is the probability of the spinner landing on $C$? Express your answer as a common fraction.',\n",
    " 'level': 'Level 1',\n",
    " 'type': 'Counting & Probability',\n",
    " 'solution': 'The spinner is guaranteed to land on exactly one of the three regions, so we know that the sum of the probabilities of it landing in each region will be 1. If we let the probability of it landing in region $C$ be $x$, we then have the equation $1 = \\\\frac{5}{12}+\\\\frac{1}{3}+x$, from which we have $x=\\\\boxed{\\\\frac{1}{4}}$.'}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f739d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask for GPT's completion to 'problem' column\n",
    "\n",
    "# Compare to dataset 'solution' column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ef574",
   "metadata": {},
   "source": [
    "# From Experiment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11b9da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35c956a0-610e-48b7-8b62-bd9e7c6e1673",
   "metadata": {},
   "source": [
    "When the ai considers whether to step back or not, does its one-word answer differ from its CoT answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7b634e-49ae-4fe0-8426-2b405c6828b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import sample\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    context: sample.Context\n",
    "    one_token_cpc_result: str\n",
    "    cot_cpc_result: str\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from llm import LLM\n",
    "from openai import OpenAI\n",
    "llm = LLM(OpenAI(), \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a40399-943e-4679-8cd9-44a330734731",
   "metadata": {},
   "source": [
    "Each 'passage' is a lengthy text where we are reasoning through a problem.\n",
    "Consider progressively larger context parts of each passage (that is, we are 'checking in' as we proceed through reading the passage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27fbda6e-a21a-4426-85b5-e426b333dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "passages = json.load(open(\"data/passages1.json\"))\n",
    "checkpoints = (text for document in passages for text in sample.checkpoints(document, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae847f-5fb8-4fa9-9982-594486ff8473",
   "metadata": {},
   "source": [
    "and for each context part, ask the llm if the current approach is working or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93914eb5-481e-4224-88fc-29dc429344a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver import perform_one_token_cpc, perform_cot_cpc\n",
    "\n",
    "results = [\n",
    "    Result(\n",
    "        context=context,\n",
    "        one_token_cpc_result=perform_one_token_cpc(llm, context),\n",
    "        cot_cpc_result=perform_cot_cpc(llm, context)\n",
    "    )\n",
    "    for context in checkpoints\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58355b-04de-4411-a318-c322e7e0c24e",
   "metadata": {},
   "source": [
    "For each result, determine whether the result is good (the two cpc methods agreed) or bad (they disagreed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b1fefe9-fa91-498c-a20b-9b98121f0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver import perform_one_token_cpc, perform_cot_cpc\n",
    "from judge import JudgeResult\n",
    "\n",
    "evaluations = [\n",
    "    JudgeResult(\n",
    "        result=result,\n",
    "        score=1.0 if result.one_token_cpc_result.lower() == result.cot_cpc_result.lower() else 0.0\n",
    "    )\n",
    "    for result in results\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94059950-8c6e-40cb-ac25-86f729ded47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For context ending in '...sonal and individualized concept that each person must discover and define for themselves.'\n",
      "one_token_cpc_result=Adapt, cot_cpc_result=, score=0.0\n"
     ]
    }
   ],
   "source": [
    "for evaluation in evaluations[:10]:\n",
    "    print(f\"For context ending in '...{evaluation.result.context.text[-90:]}'\")\n",
    "    print(f\"one_token_cpc_result={evaluation.result.one_token_cpc_result}, \"\n",
    "          f\"cot_cpc_result={evaluation.result.cot_cpc_result}, score={evaluation.score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
