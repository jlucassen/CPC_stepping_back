{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from termcolor import colored\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from llm import LLM\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "gpt35t = LLM(\"gpt-3.5-turbo\")\n",
    "gpt4 = LLM(\"gpt-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make quadratic problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_quadratic_problems import make_quadratic_problem\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "llm = LLM(\"gpt-3.5-turbo\")\n",
    "\n",
    "for max in range(5, 50, 5):\n",
    "    for factorable in [True, False]:\n",
    "        filename = f'data/quadratic_problems/quadratic_problems_{max}_{factorable}.jsonl'\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"Making problems ({max=}, {factorable=}), saving to {filename}\", 'blue')\n",
    "            with open(f'data/quadratic_problems/quadratic_problems_{max}_{factorable}.jsonl', 'w') as outfile:\n",
    "                for _ in range(100):\n",
    "                    outfile.write(make_quadratic_problem(max, factorable) + '\\n')\n",
    "        else:\n",
    "            print(colored(f\"Skipping making problems ({max=}, {factorable=}) because {filename} already exists\", 'blue'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make quadratic-solving contexts with GPT-3.5-turbo and GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_quadratic_contexts import solve_quadratic_problems\n",
    "\n",
    "# prompt is an unprincipled DoF here.\n",
    "# pros: makes expected switching behavior clear\n",
    "# cons: model is still in \"assisstant mode\", not \"trying to solve the problem as efficiently as possible mode\"\n",
    "prompt = \"Please find the roots of the quadratic equation {equation}. Start by trying to factor the equation. If you can't factor it, then use the quadratic formula. If you factor the equation successfully, do not use the quadratic formula.\"\n",
    "\n",
    "# remember to force feed the model so it starts by attempting factoring! Otherwise switch rate will be too low.\n",
    "false_start = \"First, I'll try solving this equation by factoring.\"\n",
    "\n",
    "for llm in [gpt35t, gpt4]:\n",
    "    for problem_filename in os.listdir('data/quadratic_problems'):\n",
    "        outfile = f'data/quadratic_contexts_{llm.model_name}/' + str.replace(problem_filename, 'problem', 'context')\n",
    "        if not os.path.exists(outfile):\n",
    "            print(f\"Solving problems in {problem_filename}, writing to {outfile}\")\n",
    "            solve_quadratic_problems(problem_filename, prompt, false_start, llm)\n",
    "        else:\n",
    "            print(colored(f\"Skipping solving problems in {problem_filename} because {outfile} already exists\", 'blue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dir in ['data/quadratic_contexts_gpt-3.5-turbo/', 'data/quadratic_contexts_gpt-4/']:\n",
    "    for filename in os.listdir(dir):\n",
    "        split_name = filename.split('_')\n",
    "        difficulty = split_name[-2]\n",
    "        is_factorizable = split_name[-1].split('.')[0]\n",
    "        with open(dir+'/'+filename, 'r') as f:\n",
    "            v = np.mean(['formula' in line for line in f.readlines()])\n",
    "            plt.plot(int(difficulty), v, 'ro' if is_factorizable == 'True' else 'bo')\n",
    "    plt.title(dir)\n",
    "    plt.xlabel('difficulty parameter')\n",
    "    plt.ylabel('proportion of problems solved with formula')\n",
    "    plt.ylim(0, 1)\n",
    "    red_dot, = plt.plot([], [], 'ro', label='factorizable')\n",
    "    blue_dot, = plt.plot([], [], 'bo', label='not factorizable')\n",
    "    plt.legend(handles=[red_dot, blue_dot])\n",
    "    plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split contexts into prefixes, judge switching, and load into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_switch_utils import make_non_decreasing, original_4\n",
    "\n",
    "pref_res = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/passages_35t.csv'):\n",
    "    print(\"Making passages_35t\")\n",
    "    passages_35t = []\n",
    "    for filename in tqdm(os.listdir('data/quadratic_contexts_gpt-3.5-turbo')):\n",
    "        # filename has format quadratic_contexts_[difficulty]_[is_factorizable].jsonl\n",
    "        # Each jsonl file is a list of objects, one json object per line. each object has 'equation' and 'context' field\n",
    "        split_name = filename.split('_')\n",
    "        difficulty = split_name[-2]\n",
    "        is_factorizable = split_name[-1].split('.')[0]\n",
    "\n",
    "        with open('data/quadratic_contexts_gpt-3.5-turbo/'+filename, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line_contents = json.loads(line)\n",
    "                prefixes = [line_contents['context'][:pref_end] for pref_end in range(pref_res, len(line), pref_res)]\n",
    "                measured_switching = [original_4(line_contents['context'], prefix) for prefix in prefixes]\n",
    "                processed_switching = make_non_decreasing(measured_switching)\n",
    "                switch_index = processed_switching.index(1) if 1 in processed_switching else len(processed_switching[0])\n",
    "                for i, prefix in enumerate(prefixes):\n",
    "                    passages_35t.append({\n",
    "                        'prefix': prefix,\n",
    "                        'difficulty': difficulty,\n",
    "                        'is_factorizable': 'Yes' if is_factorizable == 'True' else 'No',\n",
    "                        'context': line_contents['context'],\n",
    "                        'equation': line_contents['equation'],\n",
    "                        'measured_switching': measured_switching,\n",
    "                        'processed_switching': processed_switching,\n",
    "                        'index': i,\n",
    "                        'switch_index': switch_index\n",
    "                    })\n",
    "    passages_35t = pd.DataFrame(passages_35t)\n",
    "    passages_35t.to_csv('data/passages_35t.csv')\n",
    "else:\n",
    "    print(colored(\"Skipping making passages_35t because it already exists\", 'blue'))\n",
    "    passages_35t = pd.read_csv('data/passages_35t.csv')\n",
    "    passages_35t['measured_switching'] = passages_35t['measured_switching'].apply(lambda x: [int(x) for x in x.strip(\"[]\").split('], [')[0].split(\", \")])\n",
    "    passages_35t['processed_switching'] = passages_35t['processed_switching'].apply(lambda x: [int(x) for x in x.strip(\"[]\").split('], [')[0].split(\", \")])\n",
    "    passages_35t['switch_index'] = passages_35t['processed_switching'].apply(lambda x: x.index(1) if 1 in x else len(x))\n",
    "\n",
    "if not os.path.exists('data/passages_4.csv'):\n",
    "    print(\"Making passages_4\")\n",
    "    passages_4 = []\n",
    "    for filename in tqdm(os.listdir('data/quadratic_contexts_gpt-4')):\n",
    "        # filename has format quadratic_contexts_[difficulty]_[is_factorizable].jsonl\n",
    "        # Each jsonl file is a list of objects, one json object per line. each object has 'equation' and 'context' field\n",
    "        split_name = filename.split('_')\n",
    "        difficulty = split_name[-2]\n",
    "        is_factorizable = split_name[-1].split('.')[0]\n",
    "\n",
    "        with open('data/quadratic_contexts_gpt-4/'+filename, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                line_contents = json.loads(line)\n",
    "                prefixes = [line_contents['context'][:pref_end] for pref_end in range(pref_res, len(line), pref_res)]\n",
    "                measured_switching = [original_4(line_contents['context'], prefix) for prefix in prefixes]\n",
    "                processed_switching = make_non_decreasing(measured_switching)\n",
    "                switch_index = processed_switching.index(1) if 1 in processed_switching else len(processed_switching[0])\n",
    "                for i, prefix in enumerate(prefixes):\n",
    "                    passages_4.append({\n",
    "                        'prefix': prefix,\n",
    "                        'difficulty': difficulty,\n",
    "                        'is_factorizable': 'Yes' if is_factorizable == 'True' else 'No',\n",
    "                        'context': line_contents['context'],\n",
    "                        'equation': line_contents['equation'],\n",
    "                        'measured_switching': measured_switching,\n",
    "                        'processed_switching': processed_switching,\n",
    "                        'index': i,\n",
    "                        'switch_index': switch_index\n",
    "                    })\n",
    "    passages_4 = pd.DataFrame(passages_4)\n",
    "    passages_4.to_csv('data/passages_4.csv')\n",
    "else:\n",
    "    print(colored(\"Skipping making passages_4 because it already exists\", 'blue'))\n",
    "    passages_4 = pd.read_csv('data/passages_4.csv')\n",
    "    passages_4['measured_switching'] = passages_4['measured_switching'].apply(lambda x: [int(x) for x in x.strip(\"[]\").split('], [')[0].split(\", \")])\n",
    "    passages_4['processed_switching'] = passages_4['processed_switching'].apply(lambda x: [int(x) for x in x.strip(\"[]\").split('], [')[0].split(\", \")])\n",
    "    passages_4['switch_index'] = passages_4['processed_switching'].apply(lambda x: x.index(1) if 1 in x else len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_4['processed_switching'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(passages_4['switch_index'])\n",
    "plt.show()\n",
    "plt.hist(passages_35t['switch_index'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assess 1t and COT CPC for prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures as futures\n",
    "from solver import perform_one_token_cpc, perform_cot_cpc\n",
    "\n",
    "def process_row(llm, row):\n",
    "    row['cpc_one_token'] = perform_one_token_cpc(llm, row['prefix'])\n",
    "    cot, answer = perform_cot_cpc(llm, row['prefix'])\n",
    "    row['cpc_cot_thoughts'] = cot\n",
    "    row['cpc_cot_answer'] = answer\n",
    "    return row\n",
    "\n",
    "def experiment2(llm, passages: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes the rows in the given dataframe which have not already been processed.\n",
    "\n",
    "    :dataframe: a dataframe with at least columns 'equation', 'difficulty', 'is_factorizable', and 'context'.\n",
    "    Some rows may already have been processed, in which case they will have 'one_token_cpc_result' and 'cot_cpc_result'\n",
    "    columns, or else a value in the 'error' column.\n",
    "    :return: dataframe with the results of processing all rows in the given `passages` dataframe\n",
    "    \"\"\"\n",
    "    # turn passages into a rows dict including index (id, numerical)\n",
    "    rows = []\n",
    "    try:\n",
    "        with futures.ThreadPoolExecutor() as executor:\n",
    "            row_futures = [executor.submit(process_row, llm, row) for row in passages.to_dict(orient='records')]\n",
    "            for future in tqdm(futures.as_completed(row_futures), total=len(row_futures)):\n",
    "                if future.exception():\n",
    "                    print(f'Error processing row: {type(future.exception())} {str(future.exception())}')\n",
    "                    continue\n",
    "                rows.append(future.result())\n",
    "    except Exception as e:\n",
    "        print(f'Error processing rows: {type(e)} {str(e)}')\n",
    "    finally:\n",
    "        return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/full_data_35t.csv'):\n",
    "    print(\"Making full_data_35t\")\n",
    "    full_data_35t = experiment2(gpt35t, passages_35t)\n",
    "    full_data_35t.to_csv('data/full_data_35t.csv')\n",
    "else:\n",
    "    print(colored(\"Skipping making full_data_35t because it already exists\", 'blue'))\n",
    "    full_data_35t = pd.read_csv('data/full_data_35t.csv')\n",
    "\n",
    "if not os.path.exists('data/full_data_4.csv'):\n",
    "    print(\"Making full_data_4\")\n",
    "    full_data_4 = experiment2(gpt4, passages_4)\n",
    "    full_data_4.to_csv('data/full_data_4.csv')\n",
    "else:\n",
    "    print(colored(\"Skipping making full_data_4 because it already exists\", 'blue'))\n",
    "    full_data_4 = pd.read_csv('data/full_data_4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save results and analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Convert columns to numeric\n",
    "full_data_35t['dist_to_switch'] = full_data_35t['index'] - full_data_35t['switch_index']\n",
    "full_data_35t['cpc_one_token_numeric'] = full_data_35t['cpc_one_token'].map({'Yes': 1, 'No': 0})\n",
    "full_data_35t['cpc_cot_answer_numeric'] = full_data_35t['cpc_cot_answer'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Group the data and calculate mean and sem\n",
    "grouped = full_data_35t.groupby('dist_to_switch')[['cpc_one_token_numeric']].agg(['mean', 'count', sem]).reset_index()\n",
    "grouped_cot = full_data_35t.groupby('dist_to_switch')[['cpc_cot_answer_numeric']].agg(['mean', 'count', sem]).reset_index()\n",
    "\n",
    "# Calculate confidence intervals\n",
    "grouped['cpc_one_token_numeric_ci_low'] = grouped['cpc_one_token_numeric', 'mean'] - 1.96 * grouped['cpc_one_token_numeric', 'sem']# / np.sqrt(grouped['cpc_one_token_numeric', 'count'])\n",
    "grouped['cpc_one_token_numeric_ci_high'] = grouped['cpc_one_token_numeric', 'mean'] + 1.96 * grouped['cpc_one_token_numeric', 'sem']# / np.sqrt(grouped['cpc_one_token_numeric', 'count'])\n",
    "\n",
    "grouped_cot['cpc_cot_ci_low'] = grouped_cot['cpc_cot_answer_numeric', 'mean'] - 1.96 * grouped_cot['cpc_cot_answer_numeric', 'sem']# / np.sqrt(grouped_cot['cpc_cot_answer_numeric', 'count'])\n",
    "grouped_cot['cpc_cot_ci_high'] = grouped_cot['cpc_cot_answer_numeric', 'mean'] + 1.96 * grouped_cot['cpc_cot_answer_numeric', 'sem']# / np.sqrt(grouped_cot['cpc_cot_answer_numeric', 'count'])\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(grouped['dist_to_switch'], grouped['cpc_one_token_numeric', 'mean'], yerr=[grouped['cpc_one_token_numeric', 'mean'] - grouped['cpc_one_token_numeric_ci_low'], grouped['cpc_one_token_numeric_ci_high'] - grouped['cpc_one_token_numeric', 'mean']], fmt='o', capsize=5, label='CPC One Token')\n",
    "plt.errorbar(grouped_cot['dist_to_switch'], grouped_cot['cpc_cot_answer_numeric', 'mean'], yerr=[grouped_cot['cpc_cot_answer_numeric', 'mean'] - grouped_cot['cpc_cot_ci_low'], grouped_cot['cpc_cot_ci_high'] - grouped_cot['cpc_cot_answer_numeric', 'mean']], fmt='o', capsize=5, color='r', label='CPC COT')\n",
    "plt.vlines(0, 0, 1, linestyles='dashed', colors='gray')\n",
    "plt.xlabel('Distance to Switch')\n",
    "plt.ylabel('Average Value')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import sem\n",
    "\n",
    "# Convert columns to numeric\n",
    "full_data_4['dist_to_switch'] = full_data_4['index'] - full_data_4['switch_index']\n",
    "full_data_4['cpc_one_token_numeric'] = full_data_4['cpc_one_token'].map({'Yes': 1, 'No': 0})\n",
    "full_data_4['cpc_cot_answer_numeric'] = full_data_4['cpc_cot_answer'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Group the data and calculate mean and sem\n",
    "grouped = full_data_4.groupby('dist_to_switch')[['cpc_one_token_numeric']].agg(['mean', 'count', sem]).reset_index()\n",
    "grouped_cot = full_data_4.groupby('dist_to_switch')[['cpc_cot_answer_numeric']].agg(['mean', 'count', sem]).reset_index()\n",
    "\n",
    "# Calculate confidence intervals\n",
    "grouped['cpc_one_token_numeric_ci_low'] = grouped['cpc_one_token_numeric', 'mean'] - 1.96 * grouped['cpc_one_token_numeric', 'sem']# / np.sqrt(grouped['cpc_one_token_numeric', 'count'])\n",
    "grouped['cpc_one_token_numeric_ci_high'] = grouped['cpc_one_token_numeric', 'mean'] + 1.96 * grouped['cpc_one_token_numeric', 'sem']# / np.sqrt(grouped['cpc_one_token_numeric', 'count'])\n",
    "\n",
    "grouped_cot['cpc_cot_ci_low'] = grouped_cot['cpc_cot_answer_numeric', 'mean'] - 1.96 * grouped_cot['cpc_cot_answer_numeric', 'sem']# / np.sqrt(grouped_cot['cpc_cot_answer_numeric', 'count'])\n",
    "grouped_cot['cpc_cot_ci_high'] = grouped_cot['cpc_cot_answer_numeric', 'mean'] + 1.96 * grouped_cot['cpc_cot_answer_numeric', 'sem']# / np.sqrt(grouped_cot['cpc_cot_answer_numeric', 'count'])\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(grouped['dist_to_switch'], grouped['cpc_one_token_numeric', 'mean'], yerr=[grouped['cpc_one_token_numeric', 'mean'] - grouped['cpc_one_token_numeric_ci_low'], grouped['cpc_one_token_numeric_ci_high'] - grouped['cpc_one_token_numeric', 'mean']], fmt='o', capsize=5, label='CPC One Token')\n",
    "plt.errorbar(grouped_cot['dist_to_switch'], grouped_cot['cpc_cot_answer_numeric', 'mean'], yerr=[grouped_cot['cpc_cot_answer_numeric', 'mean'] - grouped_cot['cpc_cot_ci_low'], grouped_cot['cpc_cot_ci_high'] - grouped_cot['cpc_cot_answer_numeric', 'mean']], fmt='o', capsize=5, color='r', label='CPC COT')\n",
    "plt.vlines(0, 0, 1, linestyles='dashed', colors='gray')\n",
    "plt.xlabel('Distance to Switch')\n",
    "plt.ylabel('Average Value')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "censored-cognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
